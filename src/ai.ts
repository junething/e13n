import { Mistral } from '@mistralai/mistralai';
import { NamedString, Namer, Preflight, PreflightResult } from "./app";
import { sleep, takeAsync, throwErr } from './lib';
import { Options, StringContext } from './app';
import { Queue } from './collections';

const API_KEY = process.env.MISTRAL_API_KEY;
const QUERY_BATCH_SIZE = 4;
const REQUEST_INTERVAL = 1600;

const client = new Mistral({ apiKey: API_KEY });


type LLMResponse = Record<string, {
  name?: string,
  confidence?: number
}>;
const askLLM = async <K,>(
  cntxs: [K, StringContext][],
  options: Options,
  attempt = 1
): Promise<[K, NamedString][]> => {
  const json = Object.fromEntries(cntxs.map(([_, c]) => (
    [c.text, {
      context: c.suroundingCode
    }]
  )));
  const request = await client.agents.complete({
    agentId: "ag:528ca108:20250228:naming-agent:4b20f6d7",
    messages: [{
      role: 'user',
      content: JSON.stringify(json)
    }]
  });
  const aiMistake = (response: any, _text: string, _attempt: number): never => {
    console.error(response);
    throw new Error("API or LLM Error: Exceeded max retries");
  }
  const stringResponse = request.choices?.[0]?.message.content?.toString();
  // Partial because JSON generated by LLM and may be wrong.
  const response: Partial<LLMResponse> = JSON.parse(stringResponse ?? throwErr(""));

  return cntxs.map(([k, v]) => {
    const answer = response[v.text] ?? aiMistake(response, v.text, attempt);
    const name = answer.name ?? aiMistake(response, v.text, attempt);
    const confidence = answer.confidence ?? aiMistake(response, v.text, attempt);
    return ([k, {
      name,
      moveConfidence: confidence,
      sugestion: v.sugestion ?? name,
      ...v
    }] as [K, NamedString])
  });
};
const newLimiter = <P, F extends (...args: any[]) => Promise<P>>(
  fn: F,
  interval: number
): ((...args: Parameters<F>) => Promise<P>) => {
  let lastExecution = 0;

  return async (...args: Parameters<F>): Promise<P> => {
    const delay = Math.max(0, interval - (Date.now() - lastExecution));
    //console.log(`Waiting ${delay}`, lastExecution, Date.now());
    lastExecution += interval;
    await new Promise((resolve) => setTimeout(resolve, delay));
    //console.log("Done");
    lastExecution = Date.now();
    return fn(...args);
  };
}

async function* generator<K,>(provider: AsyncGenerator<[K, StringContext]>, options: Options): AsyncGenerator<[K, NamedString]> {
  const queue = new Queue<[K, NamedString]>();
  const limitedAskLLM: typeof askLLM = newLimiter(askLLM, REQUEST_INTERVAL);
  let promise: Promise<[K, NamedString][]> | undefined = undefined;
  while (true) {
    if (queue.count() < 3) {
      const batch = await takeAsync(provider, QUERY_BATCH_SIZE);
      if (batch.length == 0) {
        return;
      }
      promise = limitedAskLLM(batch, options).then(b => {
        queue.push(...b);
        return b;
      });
      if (queue.count() == 0)
        await promise;
      let val = queue.take();
      if (val !== undefined) {
        yield val;
      } else {
        return;
      }
    } else {
      yield queue.take()!
    }
  }
};

const preflight: Preflight = async (): Promise<PreflightResult> => {
  if (!API_KEY) {
    return {
      good: false,
      errors: ["NO API KEY: The env var MISTRAL_API_KEY is not set."]
    }
  }
  // TODO: Add check that LLM is working
  return {
    good: true
  }
}
export const namer: Namer = {
  name: 'ai',
  preflight,
  namer: generator
}
