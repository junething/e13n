import { Mistral } from '@mistralai/mistralai';
import { NamedString, Namer, Preflight, PreflightResult } from "./app";
import { take, takeAsync, throwErr } from './lib';
import { Options, StringContext } from './app';
import { Queue } from './collections';
import { getCodeLines } from './main';

const API_KEY = process.env.MISTRAL_API_KEY;
const AGENT_ID = "ag:528ca108:20250228:naming-agent:4b20f6d7"
const QUERY_BATCH_SIZE = 4;
const REQUEST_INTERVAL = 1600;

const client = new Mistral({ apiKey: API_KEY });

type LLMResponse = Record<string, {
  name?: string,
  confidence?: number
}>;

const askLLM = (options: Options) => async <K,>(cntxs: [K, StringContext][], attempt = 1): Promise<[K, NamedString][]> => {
  const json = Object.fromEntries(cntxs.map(([_, c]) => (
    [c.text, {
      context: getCodeLines(c.suroundingCode, options.linesOfContext).join("\n")
    }]
  )));
  //console.log(JSON.stringify(json));
  const request = await client.agents.complete({
    agentId: AGENT_ID,
    messages: [{
      role: 'user',
      content: JSON.stringify(json)
    }]
  });

  const aiMistake = (response: any, _text: string, _attempt: number): never => {
    console.error(response);
    throw new Error("API or LLM Error: Exceeded max retries");
  }
  const stringResponse = request.choices?.[0].message.content?.toString();
  // Partial because JSON generated by LLM may be wrong.
  //console.log(stringResponse);
  const response: Partial<LLMResponse> = JSON.parse(stringResponse ?? throwErr(""));

  return cntxs.map(([k, v]) => {
    const answer = response[v.text] ?? aiMistake(response, v.text, attempt);
    const name = answer.name ?? aiMistake(response, v.text, attempt);
    const confidence = answer.confidence ?? aiMistake(response, v.text, attempt);
    return ([k, {
      name,
      moveConfidence: confidence,
      sugestion: v.sugestion ?? name,
      ...v
    }] as [K, NamedString])
  });
};
const newLimiter = <P, F extends (...args: any[]) => Promise<P>>(
  fn: F,
  interval: number
): ((...args: Parameters<F>) => Promise<P>) => {
  let lastExecution = 0;

  return async (...args: Parameters<F>): Promise<P> => {
    const delay = Math.max(0, interval - (Date.now() - lastExecution));
    //console.log(`Waiting ${delay}`, lastExecution, Date.now());
    lastExecution += interval;
    await new Promise((resolve) => setTimeout(resolve, delay));
    //console.log("Done");
    lastExecution = Date.now();
    return fn(...args);
  };
}

async function* generator<K,>(provider: AsyncGenerator<[K, StringContext]>, options: Options): AsyncGenerator<[K, NamedString]> {
  const queue = new Queue<[K, NamedString]>();
  const limitedAskLLM: ReturnType<typeof askLLM> = newLimiter(askLLM(options), REQUEST_INTERVAL);
  //const limitedAskLLM = askLLM(options);
  let promise: Promise<[K, NamedString][]> | undefined = undefined;
  while (true) {
    //let b = await takeAsync(provider, 1);
    //let a = await askLLM(options)(b);
    //yield a[0];
    if (queue.count() < QUERY_BATCH_SIZE - 1) {
      const batch = await takeAsync(provider, QUERY_BATCH_SIZE);
      if (batch.length != 0) {
        promise = limitedAskLLM(batch).then(b => {
          queue.push(...b);
          return b;
        });
        if (queue.count() == 0) {
          await promise;
          yield queue.take()!;
        }
      } else if (batch.length == 0 && queue.count() == 0) {
        return;
      }
    }
    yield queue.take()!;
  }
};

const preflight: Preflight = async (): Promise<PreflightResult> => {
  if (!API_KEY) {
    return {
      good: false,
      errors: ["NO API KEY: The env var MISTRAL_API_KEY is not set."]
    }
  }
  // TODO: Add check that LLM is working
  return {
    good: true
  }
}
export const namer: Namer = {
  name: 'ai',
  preflight,
  namer: generator
}
